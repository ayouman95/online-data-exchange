#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import logging
import time
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
import pycountry
import gzip
from io import BytesIO

from qcloud_cos import CosConfig, CosS3Client
import oss2


# ================== é…ç½®åŒº ==================
TENCENT_SECRET_ID = os.environ['COS_SECRET_ID']
TENCENT_SECRET_KEY = os.environ['COS_SECRET_KEY']
SELECT_REGION = os.environ.get('SELECT_REGION').lower()
TENCENT_APPID = "1374116111"
COS_REGION_MAP = {
    'de': 'eu-frankfurt',
    'sg': 'ap-singapore',
    'us': 'na-siliconvalley'
}

# é˜¿é‡Œäº‘ OSS é…ç½®ï¼ˆè·¯å¾„æ—¶é—´åŸºäº UTC+0ï¼‰
ALI_ACCESS_KEY_ID = os.environ["OSS_ACCESS_KEY_ID"]
ALI_ACCESS_KEY_SECRET = os.environ["OSS_ACCESS_KEY_SECRET"]
ALI_OSS_ENDPOINT = "https://oss-ap-southeast-1.aliyuncs.com"  # æ›¿æ¢ä¸ºä½ å®é™…çš„ endpoint
ALI_BUCKET_NAME = "tracking-collect-data-test"  # TODO: æ”¹æˆå®é™…çš„

# æ–‡ä»¶å¤§å°é™åˆ¶ï¼ˆå•ä½ï¼šMBï¼‰ï¼Œkey ä¸º (platform, geo3_lower)ï¼Œvalue ä¸ºæœ€å¤§ MB æ•°
# ç¤ºä¾‹ï¼šè¾¾åˆ°é™åˆ¶åä¸å†è¿½åŠ æ•°æ®ï¼Œç›´æ¥ä¸Šä¼ å·²æœ‰éƒ¨åˆ†
SIZE_LIMITS_MB = {
    ('android', 'are'): 10,
    ('android', 'bra'): 90,
    ('android', 'can'): 10,
    ('android', 'deu'): 10,
    ('android', 'esp'): 30,
    ('android', 'fra'): 10,
    ('android', 'gbr'): 10,
    ('android', 'idn'): 100,
    ('android', 'ind'): 20,
    ('android', 'ita'): 10,
    ('android', 'kor'): 10,
    ('android', 'mex'): 70,
    ('android', 'nld'): 10,
    ('android', 'phl'): 80,
    ('android', 'pol'): 10,
    ('android', 'rus'): 100,
    ('android', 'sau'): 30,
    ('android', 'tha'): 40,
    ('android', 'ukr'): 10,
    ('android', 'usa'): 60,
}

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
)
logger = logging.getLogger(__name__)


def country_2to3_lower(cc2):
    try:
        country = pycountry.countries.get(alpha_2=cc2)
        return country.alpha_3.lower() if country else "xxx"
    except Exception:
        return "xxx"


def get_time_ranges_for_previous_hour():
    now_utc = datetime.utcnow()
    prev_hour_utc = now_utc.replace(minute=0, second=0, microsecond=0) - timedelta(hours=1)

    # UTC+0 è¾“å‡ºè·¯å¾„ç”¨
    utc0_hour_dt = prev_hour_utc

    # UTC+8 ç”¨äºæ„å»ºè…¾è®¯ COS è·¯å¾„
    utc8_now = now_utc + timedelta(hours=8)
    utc8_prev_hour = utc8_now.replace(minute=0, second=0, microsecond=0) - timedelta(hours=1)

    return utc0_hour_dt, utc8_prev_hour


# ==== æ ¸å¿ƒï¼šä½å†…å­˜ä¸Šä¼ å™¨ ====
class BufferedUploader:
    def __init__(self, platform, geo3, limit_mb, oss_bucket, date_part, hour_part):
        self.platform = platform
        self.geo3 = geo3
        self.limit_bytes = limit_mb * 1024 * 1024 if limit_mb else None
        self.oss_bucket = oss_bucket
        self.date_part = date_part
        self.hour_part = hour_part

        # å‹ç¼©æµ
        self.buffer = BytesIO()
        self.gz_file = gzip.GzipFile(mode='wb', fileobj=self.buffer)
        self.current_size = 0
        self.line_count = 0

        self.uploaded = False

    def write(self, line: str):
        if self.uploaded:
            return

        line_bytes = (line + '\n').encode('utf-8')
        added_size = len(line_bytes)

        # æ£€æŸ¥æ˜¯å¦è¶…é™
        if self.limit_bytes and self.current_size + added_size > self.limit_bytes:
            logger.info(f"âš ï¸ è¾¾åˆ°é™åˆ¶: {self.platform}.{self.geo3}.log.gz ({self.line_count} è¡Œ, {self.current_size} å­—èŠ‚)")
            self._flush()  # è¾¾åˆ°é™åˆ¶ï¼Œç«‹å³ä¸Šä¼ 
            return

        self.gz_file.write(line_bytes)
        self.current_size += added_size
        self.line_count += 1

    def _flush(self):
        if self.line_count == 0 or self.uploaded:
            return

        # å…³é—­å‹ç¼©æµ
        self.gz_file.close()

        # æ„é€  OSS è·¯å¾„
        filename = f"{self.platform}.{self.geo3}.log.gz"
        key = f"track/{self.date_part}/{self.hour_part}/{filename}"

        # ä¸Šä¼ 
        try:
            self.buffer.seek(0)
            self.oss_bucket.put_object(key, self.buffer)
            logger.info(f"âœ… ä¸Šä¼ å®Œæˆ: {key} ({self.line_count} è¡Œ, {self.current_size} å­—èŠ‚)")
            self.uploaded = True
        except Exception as e:
            logger.error(f"âŒ ä¸Šä¼ å¤±è´¥ {key}: {e}")
        finally:
            self.buffer.close()


# ==== ä¸»å‡½æ•° ====
def transform_line(data, geo3):
    os = data.get("platform", "")
    osi = 0
    if os == "android":
        osi = 2
    if os == "ios":
        osi = 1

    fields = [
        geo3.upper(),
        osi,
        data.get("display_manager", ""),
        data.get("deviceId", ""),
        data.get("brand", ""),
        data.get("user_agent", ""),
        data.get("ip", ""),
        data.get("language", ""),
        data.get("timestamp", ""),
        data.get("os_version", ""),
        data.get("app_id", ""),
        data.get("model", ""),
        data.get("network_type", "")
    ]

    # æ‹¼æ¥ä¸º @ åˆ†éš”å­—ç¬¦ä¸²
    output_line = "@".join(str(f) for f in fields)

    return output_line


def main():
    utc0_hour_dt, utc8_hour_dt = get_time_ranges_for_previous_hour()
    logger.info(f"UTC+8 æ—¶é—´æ®µ: {utc8_hour_dt.strftime('%Y-%m-%d %H:00')} (è¯»å–è…¾è®¯ COS)")
    logger.info(f"UTC+0 æ—¶é—´æ®µ: {utc0_hour_dt.strftime('%Y-%m-%d %H:00')} (å†™å…¥é˜¿é‡Œ OSS)")

    # åˆå§‹åŒ– OSS
    auth = oss2.Auth(ALI_ACCESS_KEY_ID, ALI_ACCESS_KEY_SECRET)
    bucket = oss2.Bucket(auth, ALI_OSS_ENDPOINT, ALI_BUCKET_NAME)

    date_part = utc0_hour_dt.strftime("%Y-%m-%d")
    hour_part = utc0_hour_dt.strftime("%H")

    # æ‰€æœ‰æ´»è·ƒçš„ä¸Šä¼ å™¨
    uploaders = {}

    def get_uploader(platform, geo3):
        key = (platform, geo3)
        if key not in uploaders:
            limit_mb = SIZE_LIMITS_MB.get(key) * 5
            uploader = BufferedUploader(platform, geo3, limit_mb, bucket, date_part, hour_part)
            uploaders[key] = uploader
        return uploaders[key]

    # ===== æµå¼å¤„ç†æ‰€æœ‰æ—¥å¿—æ–‡ä»¶ =====
    prefixes = build_cos_prefixes(utc8_hour_dt.strftime("%Y%m%d"), utc8_hour_dt.strftime("%H"))
    total_files = 0
    total_lines = 0

    for conf in prefixes:
        marker = ""
        while True:
            try:
                response = conf['client'].list_objects(
                    Bucket=conf['bucket'],
                    Prefix=conf['prefix'],
                    Marker=marker
                )
                contents = response.get('Contents', [])
                if not contents:
                    break

                for item in contents:
                    key = item['Key']
                    if key.endswith('/'):
                        continue

                    total_files += 1
                    try:
                        file_stream = conf['client'].get_object(
                            Bucket=conf['bucket'], Key=key
                        )['Body'].get_raw_stream()

                        line_count = 0
                        for raw_line in file_stream:
                            line = raw_line.strip().decode('utf-8')
                            if not line:
                                continue

                            try:
                                data = json.loads(line)

                                # çŸ«æ­£å›½å®¶
                                cc2 = data.get("country_code", "").strip().upper()
                                if cc2 == "UK":
                                    cc2 = "GB"
                                platform = data.get("platform", "").strip().lower()

                                if platform not in ["android", "ios"]:
                                    continue

                                # å¦‚æœæ‰¾ä¸åˆ°ä¸‰ä½å›½å®¶ä»£ç ï¼Œåˆ™å¿½ç•¥
                                geo3 = country_2to3_lower(cc2)
                                if geo3 == 'xxx':
                                    continue

                                # æ²¡æœ‰é™åˆ¶æ–‡ä»¶å¤§å°ï¼Œåˆ™å¿½ç•¥
                                if not SIZE_LIMITS_MB.get((platform, geo3)):
                                    continue
                                uploader = get_uploader(platform, geo3)
                                new_line = transform_line(data, geo3)
                                uploader.write(new_line)  # â¬…ï¸ è¾¹è¯»è¾¹å†™ï¼

                                line_count += 1
                            except Exception:
                                pass  # å¿½ç•¥æ— æ•ˆè¡Œ

                        total_lines += line_count
                        logger.info(f"âœ… å¤„ç†å®Œæˆ: {key} ({line_count} è¡Œ)")

                    except Exception as e:
                        logger.error(f"âŒ å¤„ç†å¤±è´¥ {key}: {e}")

                if not response.get('isTruncated'):
                    break
                marker = response.get('NextMarker', "")

            except Exception as e:
                logger.error(f"âŒ åˆ—å‡ºå¤±è´¥ {conf['bucket']}/{conf['prefix']}: {e}")
                break

    # ===== æ‰€æœ‰æ–‡ä»¶è¯»å®Œåï¼Œä¸Šä¼ å‰©ä½™æœªæ»¡çš„æ–‡ä»¶ =====
    for uploader in uploaders.values():
        uploader._flush()

    logger.info(f"ğŸ“Š å¤„ç†å®Œæˆï¼šå…± {total_files} ä¸ªæ–‡ä»¶ï¼Œ{total_lines} æ¡æ—¥å¿—ã€‚")


def build_cos_prefixes(date_str, hour_str):
    prefixes = []
    bucket_name = f"pando-adx-{SELECT_REGION}-{TENCENT_APPID}"
    prefix = f"adx_device/request/{date_str}/{hour_str}/"
    region_cos = COS_REGION_MAP[SELECT_REGION]
    config = CosConfig(Region=region_cos, SecretId=TENCENT_SECRET_ID, SecretKey=TENCENT_SECRET_KEY)
    client = CosS3Client(config)
    prefixes.append({'bucket': bucket_name, 'prefix': prefix, 'client': client})
    return prefixes


if __name__ == "__main__":
    # è®¡ç®—è€—æ—¶
    start_time = time.time()
    main()
    logger.info(f"è€—æ—¶: {time.time() - start_time} ç§’")